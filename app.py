# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12yifb-kTGGoi5lB1D7ZZg96IhQfDdJfz
"""

# app.py
import io
import traceback
from flask import Flask, request, jsonify, send_from_directory, render_template
import pandas as pd
import numpy as np
import joblib
import lightgbm as lgb
from sklearn.preprocessing import LabelEncoder

app = Flask(__name__, template_folder="templates")

# -----------------------
# Configuration / Globals
# -----------------------
# Path to pre-trained model (joblib dump). The user said it's model.pkl in project root.
PRETRAINED_MODEL_PATH = "model.pkl"

# Feature set expected by the model (minimum). If more features exist they will be used if present.
DEFAULT_FEATURES = [
    "orbital_period",
    "transit_depth",
    "transit_duration",
    "planet_radius",
    "stellar_temperature"
]

# Labels mapping expected for fine-tuning (strings to integers)
LABELS = ["CONFIRMED", "CANDIDATE", "FALSE POSITIVE"]

# In-memory storage for the pre-trained and fine-tuned model(s)
# - preloaded_model: object loaded from joblib at start if available (may be sklearn wrapper or booster)
# - finetuned_booster: lightgbm.Booster created in-memory after fine-tuning (preferred for predictions if exists)
preloaded_model = None
finetuned_booster = None
label_encoder = LabelEncoder()
label_encoder.fit(LABELS)

# -----------------------
# Utility helpers
# -----------------------
def load_pretrained_model():
    """
    Load pre-trained model.pkl via joblib if exists.
    Return the model object or None.
    The model may be:
      - a scikit-learn wrapper: lightgbm.sklearn.LGBMClassifier
      - or some other artifact produced by user
    """
    global preloaded_model
    if preloaded_model is not None:
        return preloaded_model
    try:
        preloaded_model = joblib.load(PRETRAINED_MODEL_PATH)
        app.logger.info(f"Pre-trained model loaded from {PRETRAINED_MODEL_PATH}")
        return preloaded_model
    except Exception as e:
        app.logger.warning(f"Could not load pre-trained model from {PRETRAINED_MODEL_PATH}: {e}")
        preloaded_model = None
        return None

def df_select_features(df, features=DEFAULT_FEATURES):
    """
    Return a dataframe with selected features. If some expected features are missing, raise.
    If there are extra numeric features, include them as well (keeps columns present in df).
    """
    present = [c for c in features if c in df.columns]
    if len(present) < len(features):
        missing = set(features) - set(present)
        raise ValueError(f"Missing required columns: {sorted(list(missing))}")
    # Keep all numeric columns that are in DEFAULT_FEATURES plus any numeric columns present (optional)
    # But for model stability, we will return columns in EXACT order of DEFAULT_FEATURES first, then extras sorted.
    extras = [c for c in df.columns if c not in features and pd.api.types.is_numeric_dtype(df[c])]
    selected = present + sorted(extras)
    return df[selected], selected

def to_float_df(df):
    """
    Convert all columns to numeric (where possible) and return df.
    Non-convertible values become NaN.
    """
    return df.apply(pd.to_numeric, errors='coerce')

def predict_with_model(X_df):
    """
    Predict using the finetuned_booster if exists, else the preloaded_model.
    Return probabilities and labels.
    """
    global finetuned_booster, preloaded_model
    X = X_df.values.astype(float)
    # If finetuned booster exists (lightgbm.Booster), use it
    if finetuned_booster is not None:
        # Booster.predict returns num_class probabilities when pred_proba=True
        proba = finetuned_booster.predict(X)  # shape (n_samples, n_class) for multiclass
        # fine: ensure shape consistency
        if proba.ndim == 1:
            # binary case
            proba = np.vstack([1 - proba, proba]).T
    else:
        # Use preloaded_model (sklearn wrapper) if available
        if preloaded_model is None:
            raise RuntimeError("No model loaded for prediction.")
        # if the preloaded_model is a Booster saved by user (rare), try to use its predict method directly
        if isinstance(preloaded_model, lgb.basic.Booster):
            proba = preloaded_model.predict(X)
            if proba.ndim == 1:
                proba = np.vstack([1 - proba, proba]).T
        else:
            # assume scikit-learn-like estimator with predict_proba
            proba = preloaded_model.predict_proba(X)
    # Map to labels
    # If model output num_class columns > len(LABELS), we will create generic label names
    n_class = proba.shape[1]
    if n_class == len(LABELS):
        class_names = LABELS
    else:
        class_names = [f"class_{i}" for i in range(n_class)]
    pred_indices = np.argmax(proba, axis=1)
    pred_labels = [class_names[int(i)] for i in pred_indices]
    pred_probs = proba.max(axis=1).tolist()
    # Also convert full probability arrays into lists per row with class names
    proba_per_row = []
    for row in proba:
        proba_per_row.append({class_names[i]: float(row[i]) for i in range(len(row))})
    return pred_labels, pred_probs, proba_per_row

# -----------------------
# Routes
# -----------------------

@app.route("/", methods=["GET"])
def index():
    # Serve the front-end (index.html inside templates/)
    return render_template("index.html")

@app.route("/api/process", methods=["POST"])
def api_process():
    """
    Main endpoint for both prediction and fine-tuning.
    Expects multipart/form-data:
      - file: uploaded .xlsx
      - mode: 'predict' or 'retrain' (string)
    Returns JSON:
      - mode: the requested mode
      - results: list of predictions (index, predicted_label, probability, probabilities_by_class)
      - message: optional status
      - error: present only if an error occured
    """
    global finetuned_booster
    try:
        # Validate request fields
        if "file" not in request.files:
            return jsonify({"error": "Missing file in request."}), 400
        uploaded = request.files["file"]
        mode = (request.form.get("mode") or request.args.get("mode") or "predict").lower()
        if mode not in ("predict", "retrain"):
            return jsonify({"error": "Invalid mode. Use 'predict' or 'retrain'."}), 400

        # Read the uploaded file into a pandas DataFrame in-memory
        in_memory = uploaded.read()
        bytes_io = io.BytesIO(in_memory)
        # If predict mode, try to read sheet named 'Data' first (per spec) else the first sheet
        if mode == "predict":
            try:
                df = pd.read_excel(bytes_io, sheet_name="Data", engine="openpyxl")
            except Exception:
                bytes_io.seek(0)
                df = pd.read_excel(bytes_io, sheet_name=0, engine="openpyxl")
        else:  # retrain
            # For retrain mode we expect labeled dataset; try first sheet
            bytes_io.seek(0)
            df = pd.read_excel(bytes_io, sheet_name=0, engine="openpyxl")

        if df.empty:
            return jsonify({"error": "Uploaded file contains no data."}), 400

        # Ensure required columns and numeric conversion
        # For retrain: must have 'Disposition_Using_Kepler_Data' with CONFIRMED/CANDIDATE/FALSE POSITIVE
        if mode == "retrain":
            label_col = "Disposition_Using_Kepler_Data"
            if label_col not in df.columns:
                return jsonify({"error": f"Fine-tuning requires column '{label_col}'."}), 400
            # Extract X and y
            # Convert labels to uppercase strings to match expected labels
            df[label_col] = df[label_col].astype(str).str.strip().str.upper()
            # Keep only rows with valid labels
            valid_mask = df[label_col].isin(LABELS)
            if valid_mask.sum() < 2:
                return jsonify({"error": "Not enough labeled rows with valid Disposition values (need >=2)."}), 400
            df_valid = df[valid_mask].copy()
            # Select features and convert to numeric
            X_df_full = df_valid.copy()
            # remove the label column for feature selection
            X_df_full = X_df_full.drop(columns=[label_col])
            X_df_full, selected_cols = df_select_features(X_df_full, DEFAULT_FEATURES)
            X_df_full = to_float_df(X_df_full)
            if X_df_full.isnull().any(axis=None):
                # Simple strategy: drop rows with NaNs for training
                before = X_df_full.shape[0]
                mask_non_null = ~X_df_full.isnull().any(axis=1)
                X_df_full = X_df_full[mask_non_null]
                df_valid = df_valid.iloc[mask_non_null[mask_non_null].index] if False else df_valid[mask_non_null]
                after = X_df_full.shape[0]
                if after < 2:
                    return jsonify({"error": "After dropping rows with NaNs there is insufficient data to retrain."}), 400
            y_series = df_valid.loc[X_df_full.index, label_col].map(lambda s: LABELS.index(s)).astype(int)

            # At this point X_df_full (numeric) and y_series ready
            # Load pre-trained model if exists and attempt to use it as init_model for lightgbm training
            base_model = load_pretrained_model()
            init_booster = None
            params = {
                "objective": "multiclass",
                "num_class": len(LABELS),
                "learning_rate": 0.05,
                "verbosity": -1,
                "metric": "multi_logloss",
            }
            num_boost_round = 50  # small number for a quick fine-tune during hackathon; you can increase

            # If preloaded model is a sklearn wrapper (LGBMClassifier) try to extract its booster_
            if base_model is not None:
                try:
                    if hasattr(base_model, "booster_") and base_model.booster_ is not None:
                        init_booster = base_model.booster_
                        # attempt to copy params from base model if possible
                        try:
                            base_params = base_model.get_params()
                            # don't override objective/num_class here, keep our defaults
                            if "learning_rate" in base_params:
                                params["learning_rate"] = base_params.get("learning_rate") or params["learning_rate"]
                        except Exception:
                            pass
                except Exception:
                    init_booster = None

            # Prepare LightGBM dataset
            train_set = lgb.Dataset(X_df_full.values, label=y_series.values, free_raw_data=False)

            # Train with init_model if available (this will continue boosting from existing trees when possible)
            if init_booster is not None:
                # Use lightgbm.train with init_model argument
                finetuned = lgb.train(
                    params,
                    train_set,
                    num_boost_round=num_boost_round,
                    init_model=init_booster,
                    keep_training_booster=True,
                    verbose_eval=False,
                )
            else:
                finetuned = lgb.train(
                    params,
                    train_set,
                    num_boost_round=num_boost_round,
                    keep_training_booster=True,
                    verbose_eval=False,
                )

            # Store finetuned booster in memory for further predictions
            finetuned_booster = finetuned

            # Run predictions on the (original) input rows to show how model classifies them now
            pred_labels, pred_probs, proba_per_row = predict_with_model(X_df_full)

            results = []
            for idx, (lab, p, proba_map) in enumerate(zip(pred_labels, pred_probs, proba_per_row)):
                results.append({
                    "index": int(X_df_full.index[idx]),
                    "predicted_label": lab,
                    "confidence": float(p),
                    "probabilities": proba_map
                })

            return jsonify({
                "mode": "retrain",
                "message": f"Fine-tuning completed in-memory (num_boost_round={num_boost_round}).",
                "used_features": selected_cols,
                "results": results,
            })

        else:
            # Predict mode
            # Select features from Data sheet / df
            X_df, selected_cols = df_select_features(df, DEFAULT_FEATURES)
            X_df = to_float_df(X_df)
            if X_df.isnull().any(axis=None):
                # For prediction we drop NaNs rows (you could implement imputation later)
                mask = ~X_df.isnull().any(axis=1)
                X_df = X_df[mask]
                if X_df.shape[0] == 0:
                    return jsonify({"error": "All rows have NaNs in required features."}), 400

            # Ensure model is loaded
            _ = load_pretrained_model()
            # Get predictions (this function will pick finetuned_booster if present)
            pred_labels, pred_probs, proba_per_row = predict_with_model(X_df)

            results = []
            for idx, (lab, p, proba_map) in enumerate(zip(pred_labels, pred_probs, proba_per_row)):
                results.append({
                    "index": int(X_df.index[idx]),
                    "predicted_label": lab,
                    "confidence": float(p),
                    "probabilities": proba_map
                })

            return jsonify({
                "mode": "predict",
                "used_features": selected_cols,
                "results": results,
            })

    except Exception as e:
        app.logger.error(traceback.format_exc())
        return jsonify({"error": "Server error during processing.", "details": str(e)}), 500


# Simple route to check health
@app.route("/api/health", methods=["GET"])
def health():
    return jsonify({"status": "ok", "pretrained_loaded": load_pretrained_model() is not None, "finetuned_loaded": finetuned_booster is not None})


if __name__ == "__main__":
    # Try to preload model at startup (not required, but convenient)
    load_pretrained_model()
    app.run(host="0.0.0.0", port=5000, debug=True)